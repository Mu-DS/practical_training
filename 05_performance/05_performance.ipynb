{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all algorithms are created equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://preview.redd.it/bp6g76i3iyx41.jpg?width=640&crop=smart&auto=webp&s=781fd5c86c65d74554a3eba6acd1051befa67630\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: How do you develop performant programs in the least amount of time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce development Time\n",
    "- Improve performance Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Graph Databases & PyTorch (Simon)\n",
    "2. Slurm  (Giovanni)\n",
    "3. Snakemake (Giovanni)\n",
    "4. Numba  (Robin)\n",
    "5. Dask  (Robin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow / PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous monitoring of Slurm jobs (automatically updates every 2 sec):\n",
    "```bash\n",
    "watch squeue -u rccg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief note on performance in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not all algorithms are created equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://preview.redd.it/bp6g76i3iyx41.jpg?width=640&crop=smart&auto=webp&s=781fd5c86c65d74554a3eba6acd1051befa67630\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: How do you develop performant programs in the least amount of time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce development Time\n",
    "- Improve performance Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all have heard it:\n",
    "- Python is *too slow!*\n",
    "- Python cannot replace Fortran or C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance differences within Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing code as if it was C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loops\n",
    "- Accessing Data\n",
    "- Appending Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why are these slow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python interpreter goes line-by-line\n",
    "- Therefore, no loop unrolling\n",
    "- With GIL execution of each iteration sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lists, Dictionaries, etc. allow for mixed types\n",
    "- This means, they structures of pointers rather than the data itself\n",
    "- The data might not be contiguous in memory\n",
    "- This leads to memory jumping and cache misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dictionaries, etc. are initialized with a fixed size\n",
    "- If new data gets appended, this might cause an implicit copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Asymptotic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big O Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big O - or $\\mathcal{O}(n)$ - refers to the <span style=\"text-decoration: underline\">*worst-case scaling*</span> with size $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All coefficients or smaller components are not to be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1830/1*5ZLci3SuR0zM_QlZOADv8Q.jpeg\" width=640 />\n",
    "\n",
    "https://www.bigocheatsheet.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{O}(N) + \\mathcal{O}(\\log N)  =  \\mathcal{O}(N + \\log N)  =  \\mathcal{O}(N) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Theta Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big $\\Theta$ - or $\\mathcal{\\Theta}(n)$ - refers to the <span style=\"text-decoration: underline\">*average-case scaling*</span> with size $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omega Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big $\\Omega$ - or $\\mathcal{\\Omega}(n)$ - refers to the <span style=\"text-decoration: underline\">*best-case scaling*</span> with size $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, these are simplified.\n",
    "\n",
    "For a deep dive into asymptotic analysis see: https://cathyatseneca.gitbooks.io/data-structures-and-algorithms/content/analysis/notations.html\n",
    "\n",
    "(it is also a great course on data structures and algorithms!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Object Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Object Complexity\n",
    "\n",
    "https://wiki.python.org/moin/TimeComplexity\n",
    "\n",
    "https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation        | Example      | Best or Average Case | Worst Case | Note                       |\n",
    "|------------------|--------------|--------------|----------------------|----------------------------|\n",
    "| Copy             | `l.copy()`     | O(n)         | O(n)                 | Same as l[:] which is O(N) |\n",
    "| Append           | `l.append(5)`  | O(1)         | O(1)                 | |\n",
    "| Pop last         | `l.pop()`      | O(1)         | O(1)                 | same as l.pop(-1), popping at end |\n",
    "| Pop intermediate | `l.pop(n-k)`   | O(k)         | O(k)                 | |\n",
    "| Pop first        | `l.pop(0)`     | O(n)         | O(n)                 | |\n",
    "| Insert           | `l[a:b] = ...` | O(n)         | O(n)                 | |\n",
    "| Get Item         | `l[i]`         | O(1)         | O(1)                 | |\n",
    "| Set Item         | `l[i] = 0`     | O(1)         | O(1)                 | |\n",
    "| Iteration        | `for v in l:`  | O(n)         | O(n)                 | |\n",
    "| Get Slice (k=b-a)| `l[a:b]`       | O(k)         | O(k)                 | `l[1:5]:O(l)/l[:]:O(len(l)-0)=O(N)` |\n",
    "| Del Slice (k=b-a)|              | O(n)         | O(n)                 | |\n",
    "| Delete Item      | `del l[i]`/`l.remove(a)` | O(n)         | O(n)                 | |\n",
    "| Set Slice        |              | O(k+n)       | O(k+n)               | |\n",
    "| Extend (by k)    | `l.extend(k)`  | O(len(k))    | O(len(k))            | depends only on len of extension |\n",
    "| Sort             | `l.sort()`     | O(n log n)   | O(n log n)           | key/reverse doesn't change this |\n",
    "| Multiply         | `k*l`        | O(nk)        | O(nk)                | `5*l is O(N): len(l)*l is O(N**2)` |\n",
    "| min(s), max(s)   | `min(l)/max(l)| O(n)         | O(n)                 | |\n",
    "| Get Length       | `len(l)`       | O(1)         | O(1)                 | |\n",
    "| Reverse          | `l.reverse()  | O(n)         | O(n)                 | |\n",
    "| Containment      | x `in`/`not in` l |         | O(n)                 | searches list |\n",
    "| Clear            | `l.clear()`    | O(1)         | similar to l = []    | Deferred garbage collection |\n",
    "| Construction     | `list(...)`    | O(n)         | O(n)                 | depends on length of argument\n",
    "| check `==`, `!=` | `l1 == l2`     | O(n)         |\n",
    "| Remove           | `l.remove(...)`|              | On)     | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation                           | Example      | Best or Average case          | Worst Case         | notes                                      |\n",
    "|-------------------------------------|--------------|-----------------------|--------------------|--------------------------------------------|\n",
    "| Containment                         | x in s       | O(1)                  | O(n)               | compare to list/tuple - O(n)               |\n",
    "| Length                              | len(s)       | O(1)                  | O(1)               |                                            |\n",
    "| Add                                 | s.add(a)     | O(1)                  | O(1)               |\n",
    "| Remove                              | s.remove(a)  | O(1)                  | O(1)               | compare to list/tuple - O(N)\n",
    "| Discard                             | s.discard(a) | O(1)                  | O(1)               | \n",
    "| Pop                                 | s.pop()      | O(1)                  | O(1)               | compare to list - O(N)\n",
    "| Clear                               | s.clear()    | O(1)                  | O(1)               | similar to s = set()\n",
    "| Construction                        | set(n)       | O(n)                  | O(n)               |\n",
    "| check ==, !=                        | s != t       | O(min(len(s),lent(t)) | O(n)               |\n",
    "| <=/<                                | s <= t       | O(len(s1))            | O(n)               | issubset \n",
    "| >=/>                                | s >= t       | O(len(s2))            | O(n)               | issuperset s <= t == t >= s\n",
    "| Union                               | `s | t`      | O(len(s)+len(t))      | O(len(s)+len(t))   |                                            |\n",
    "| Intersection                        | `s & t`      | O(min(len(s), lent(t))| O(len(s) * len(t)) | replace \"min\" with \"max\" if t is not a set |\n",
    "| Multiple intersection               | `s1&s2&..&sn`|                       | `(n-1)*O(l)`       | l is max(len(s1),..,len(sn))               |\n",
    "| Difference                          | s - t        |                       | O(len(t))          |                    |                                            |\n",
    "| Symmetric Diff                      | s ^ t        | O(len(s))             | O(len(s) * len(t)) |   \n",
    "| Iteration                           | for v in s:  | O(N)                  |\n",
    "| Copy                                | s.copy()     | O(N)                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation    | Example                | Average Case | Amortized Worst Case |\t \n",
    "| -------------|------------------------|--------------|----------------------| \n",
    "| Copy         | `d.copy()`             | O(n)         | O(n)                 |\n",
    "| Get Item     | `d[k]`                 | O(1)         | O(n)                 |\n",
    "| Set Item     | `d[k]=v`               | O(1)         | O(n)                 |\n",
    "| Delete Item  | `del d[k]`             | O(1)         | O(n)                 |\n",
    "| Iteration    | `for k,v in d.items()` | O(n)         | O(n)                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- just-in-time (JIT) compiler for Python\n",
    "- works with: NumPy arrays, functions, and loops\n",
    "- used with simple decorators\n",
    "- with these \"all or part of your code can \\[...\\] run at native machine code speed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda install numba\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install numba\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just-in-Time Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The @jit decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def go_slow(a):\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):\n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "grid_size = (N, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(grid_size):\n",
    "    return np.random.rand(np.prod(grid_size)).reshape(grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = setup(grid_size)\n",
    "_ = go_fast(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit x = setup(grid_size)\n",
    "_ = go_slow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit x = setup(grid_size)\n",
    "_ = go_fast(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How fast can it be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- targets compilation to specific CPU\n",
    "- \"speed up varies depending on application\"\n",
    "- generally \"can be one to two orders of magnitude\" to C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahead-of-Time Compilation (Eager Compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using signatures, declaring the function leads to compilation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format: `OutputType[shapes](InputType1[shapes1], InputType2[shapes2], ....)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `void` is the return type of functions returning nothing (which actually return None when called from Python)\n",
    "- `intp` and `uintp` are pointer-sized integers (signed and unsigned, respectively)\n",
    "- `intc` and `uintc` are equivalent to C int and unsigned int integer types\n",
    "- `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64` are fixed-width integers of the corresponding bit width (signed and unsigned)\n",
    "- `float32` and `float64` are single- and double-precision floating-point numbers, respectively\n",
    "- `complex64` and `complex128` are single- and double-precision complex numbers, respectively\n",
    "- array types can be specified by indexing any numeric type, e.g. `float32[:]` for a one-dimensional single-precision array or `int8[:,:]` for a two-dimensional array of 8-bit integers.\n",
    "\n",
    "For up-to-date version see: https://numba.pydata.org/numba-doc/latest/user/jit.html#signature-specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@jit(nb.float64[:,:](nb.float64[:, :]), nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit x = setup(grid_size)\n",
    "_ = go_fast(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nopython=True`\n",
    "- uses only numba objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nogil=True`\n",
    "- removes the Global Interpreter Lock (GIL) restrictions when only using Numba objects\n",
    "- allows concurrent execution therefore advantage of multiple cores\n",
    "- requires `nopython=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cache=True`\n",
    "- saves the compiled code for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parallel=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- automatic parallelization\n",
    "- requires `nopython=True`\n",
    "- supported operations: https://numba.pydata.org/numba-doc/latest/user/parallel.html#numba-parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators provided:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `@njit` - this is an alias for `@jit(nopython=True)` as it is so commonly used!\n",
    "- `@vectorize` - produces NumPy ufunc s (with all the ufunc methods supported).\n",
    "- `@guvectorize` - produces NumPy generalized ufunc s.\n",
    "- `@stencil` - declare a function as a kernel for a stencil like operation.\n",
    "- `@jitclass` - for jit aware classes.\n",
    "- `@cfunc` - declare a function for use as a native call back (to be called from C/C++ etc).\n",
    "- `@overload` - register your own implementation of a function for use in nopython mode, e.g. @overload(scipy.special.j0).\n",
    "\n",
    "https://numba.pydata.org/numba-doc/latest/user/5minguide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra options available in some decorators:\n",
    "- `parallel = True` - enable the automatic parallelization of the function.\n",
    "- `fastmath = True` - enable fast-math behaviour for the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stencil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 2D Finite Difference Kernel as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use periodic boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Stencil decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = (4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import stencil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@stencil\n",
    "def grad_stencil(arr):\n",
    "    return arr[-1, 0] + arr[0, -1] + arr[0, 1] + arr[1, 0] - 4*arr[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = setup(grid_size)\n",
    "arr = grad_stencil(np.pad(a, 1, 'wrap'))[1:-1, 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing to Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = setup(grid_size)\n",
    "arr = scipy.signal.convolve2d(np.pad(a, 1, 'wrap'), coeffs)[2:-2, 2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = setup(grid_size)\n",
    "arr_nb = grad_stencil(np.pad(a, 1, 'wrap'))[1:-1, 1:-1]\n",
    "arr_scipy = scipy.signal.convolve2d(np.pad(a, 1, 'wrap'), coeffs)[2:-2, 2:-2]\n",
    "assert np.allclose(arr_nb, arr_scipy), f\"Failed with max error: {np.max(np.abs(arr_nb - arr_scipy))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speeding up Stencil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def grad_stencil_wrapped(arr):\n",
    "    \"\"\"simply apply the stencil\"\"\"\n",
    "    return grad_stencil(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_nb = grad_stencil_wrapped(np.pad(a, 1, 'wrap'))[1:-1, 1:-1]\n",
    "assert np.allclose(arr_nb, arr_scipy), f\"Failed with max error: {np.max(np.abs(arr_nb - arr_scipy))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = setup(grid_size)\n",
    "arr = grad_stencil_wrapped(np.pad(a, 1, 'wrap'))[1:-1, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, nogil=True, parallel=True)\n",
    "def grad_stencil_wrapped_ops(arr):\n",
    "    \"\"\"simply apply the stencil\"\"\"\n",
    "    return gradient2d(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_nb = grad_stencil_wrapped_ops(np.pad(a, 1, 'wrap'))[1:-1, 1:-1]\n",
    "assert np.allclose(arr_nb, arr_scipy), f\"Failed with max error: {np.max(np.abs(arr_nb - arr_scipy))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = setup(grid_size)\n",
    "arr = grad_stencil_wrapped_ops(np.pad(a, 1, 'wrap'))[1:-1, 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_np(a):\n",
    "    padded = np.pad(a, 1, 'wrap')\n",
    "    arr_np = (\n",
    "        padded[0:-2, 1:-1]  # above\n",
    "        + padded[1:-1, 0:-2]  # left\n",
    "        -4 * padded[1:-1, 1:-1]  # center\n",
    "        + padded[1:-1, 2:]  # right\n",
    "        + padded[2:, 1:-1]  # below\n",
    "    )\n",
    "    return arr_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_np = grad_np(a)\n",
    "assert np.allclose(arr_np, arr_scipy), f\"Failed with max error: {np.max(np.abs(arr_np - arr_scipy))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = setup(grid_size)\n",
    "grad_np(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy vectorized with Nuba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, nogil=True, parallel=True)\n",
    "def grad_np_numba(padded):\n",
    "    return (\n",
    "        padded[0:-2, 1:-1]  # above\n",
    "        + padded[1:-1, 0:-2]  # left\n",
    "        -4 * padded[1:-1, 1:-1]  # center\n",
    "        + padded[1:-1, 2:]  # right\n",
    "        + padded[2:, 1:-1]  # below\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_nb = grad_np_numba(np.pad(a, 1, 'wrap'))\n",
    "assert np.allclose(arr_nb, arr_scipy), f\"Failed with max error: {np.max(np.abs(arr_nb - arr_scipy))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = setup(grid_size)\n",
    "arr = grad_np_numba(np.pad(a, 1, 'wrap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Plots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot performance of various versions with scaling\n",
    "\n",
    "Python module: [perfplot](https://github.com/nschloe/perfplot)\n",
    "- `pip install perfplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "perfplot.show(\n",
    "    setup=lambda n: setup((n, n)),  # or setup=numpy.random.rand\n",
    "    kernels=[\n",
    "        lambda a: grad_stencil(np.pad(a, 1, 'wrap'))[1:-1, 1:-1],\n",
    "        lambda a: scipy.signal.convolve2d(np.pad(a, 1, 'wrap'), coeffs)[2:-2, 2:-2],\n",
    "        lambda a: grad_stencil_wrapped(np.pad(a, 1, 'wrap'))[1:-1, 1:-1],\n",
    "        lambda a: grad_stencil_wrapped_ops(np.pad(a, 1, 'wrap'))[1:-1, 1:-1],\n",
    "        lambda a: grad_np(a),\n",
    "        lambda a: grad_np_numba(np.pad(a, 1, 'wrap'))\n",
    "    ],\n",
    "    labels=[\"stencil\", \"scipy\", \"stencil_wrapped\", \"stencil_wrapped_ops\", \"numpy\", \"numpy_numba\"],\n",
    "    n_range=[2 ** k for k in range(3, 15)],\n",
    "    xlabel=\"(n, n)\",\n",
    "    # More optional arguments with their default values:\n",
    "    # title=None,\n",
    "    logx=\"auto\",  # set to True or False to force scaling\n",
    "    logy=\"auto\",\n",
    "    equality_check=np.allclose,  # set to None to disable \"correctness\" assertion\n",
    "    show_progress=True,\n",
    "    # colors=None,\n",
    "    # target_time_per_measurement=1.0,\n",
    "    # time_unit=\"s\",  # set to one of (\"auto\", \"s\", \"ms\", \"us\", or \"ns\") to force plot units\n",
    "    # relative_to=1,  # plot the timings relative to one of the measurements\n",
    "    # flops=lambda n: 3*n,  # FLOPS plots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize([\"float32(float32, float32)\", \"float64(float64, float64)\"], target='parallel')\n",
    "def add_parallel(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overhead makes it useless for small scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "grid_size = (N, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, = setup(grid_size), setup(grid_size)\n",
    "reference = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_vec_parallel = add_parallel(a, b)\n",
    "assert np.allclose(ab_vec_parallel, reference), f\"Failed with max error: {np.max(np.abs(ab_vec_parallel - reference))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a, b, = setup(grid_size), setup(grid_size)\n",
    "_ = add_parallel(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a, b, = setup(grid_size), setup(grid_size)\n",
    "_ = add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a, b, = setup(grid_size), setup(grid_size)\n",
    "_ = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But useful for large scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16_000\n",
    "grid_size = (N, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a, b, = setup(grid_size), setup(grid_size)\n",
    "_ = add_parallel(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a, b, = setup(grid_size), setup(grid_size)\n",
    "_ = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 10  # s\n",
    "def get_time_scaling(function_list, setup_func, setup_list, max_time):\n",
    "    t = 0\n",
    "    i = 0\n",
    "    times = {func.__name__: [] for func in function_list}\n",
    "    while t < max_time and i < len(setup_list):\n",
    "        for func in function_list:\n",
    "            # Setup\n",
    "            t0 = time.time()\n",
    "            s = setup_func(setup_list[i])\n",
    "            t1 = time.time()\n",
    "            t = max(t, t1-t0)\n",
    "            # Compute\n",
    "            t0 = time.time()\n",
    "            _ = func(*s)\n",
    "            t1 = time.time()\n",
    "            t = max(t, t1-t0)\n",
    "            times[func.__name__].append(t1-t0)\n",
    "        print(t, setup_list[i])\n",
    "        i += 1\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = get_time_scaling(\n",
    "    function_list=[add_parallel, add2],\n",
    "    setup_func=lambda x: (setup(x), setup(x)),\n",
    "    setup_list=[(2**i, 2**i) for i in np.arange(6, 14)],\n",
    "    max_time=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot([scale for scale in range(6, 14)], times['add_parallel'], label='add_parallel')\n",
    "ax.plot([scale for scale in range(6, 14)], times['add2'], label='add2')\n",
    "ax.set_xticklabels(list(map(int, 2**ax.get_xticks())))\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_parallel(a, b).dtype, (a + b).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, vectorize, float32\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = (4_000, 4_000)\n",
    "a = setup(grid_size)\n",
    "b = setup(grid_size)\n",
    "t0 = time.time()\n",
    "_ = add_parallel(a, b)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize([\"float32(float32, float32)\", \"float64(float64, float64)\"], target='cuda')\n",
    "def add_gpu(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(grid_size):\n",
    "    return np.random.rand(np.prod(grid_size)).reshape(grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data\n",
    "grid_size = (5_000, 5_000)\n",
    "dtype = np.float64\n",
    "# Create Data\n",
    "a = setup(grid_size)\n",
    "b = setup(grid_size)\n",
    "c = np.zeros(grid_size)\n",
    "# Define Time\n",
    "times = {}\n",
    "with cuda.gpus[0]:\n",
    "    # Create Arrays\n",
    "    t0 = time.time()\n",
    "    x = cuda.device_array(grid_size, dtype=dtype)  # you explicitly should type these\n",
    "    y = cuda.device_array(grid_size, dtype=dtype)  # you explicitly should type these\n",
    "    result = cuda.device_array(grid_size, dtype=dtype)  # you explicitly should type these\n",
    "    times['allocate memory'] = time.time() - t0\n",
    "    # Copy data to GPU\n",
    "    t0 = time.time()\n",
    "    x.copy_to_device(a)  # you explicitly should type these\n",
    "    y.copy_to_device(b)  # you explicitly should type these\n",
    "    times['copy to device'] = time.time() - t0\n",
    "    # Add on GPU\n",
    "    t0 = time.time()\n",
    "    result = add_gpu(x, y)\n",
    "    times['compute'] = time.time() - t0\n",
    "    # Copy data back\n",
    "    t0 = time.time()\n",
    "    x.copy_to_host(a)  # you explicitly should type these\n",
    "    y.copy_to_host(b)  # you explicitly should type these\n",
    "    result.copy_to_host(c)  # you explicitly should type these\n",
    "    times['copy to host'] = time.time() - t0\n",
    "    # cleanup\n",
    "    t0 = time.time()\n",
    "    del x, y, result  # you explicitly should type these\n",
    "    times['delete'] = time.time() - t0\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data\n",
    "dtype = np.float64\n",
    "gpu_times = {}\n",
    "for grid_size in [(2**i, 2**i) for i in np.arange(6, 15)]:\n",
    "    # Create Data\n",
    "    a = setup(grid_size)\n",
    "    b = setup(grid_size)\n",
    "    c = np.zeros(grid_size)\n",
    "    # Define Time\n",
    "    times_gpu = {}\n",
    "    with cuda.gpus[0]:\n",
    "        # Create Arrays\n",
    "        t0 = time.time()\n",
    "        x = cuda.device_array(grid_size, dtype=dtype)\n",
    "        y = cuda.device_array(grid_size, dtype=dtype)\n",
    "        result = cuda.device_array(grid_size, dtype=dtype)\n",
    "        times_gpu['allocate memory'] = time.time() - t0\n",
    "        # Copy data to GPU\n",
    "        t0 = time.time()\n",
    "        x.copy_to_device(a)\n",
    "        y.copy_to_device(b)\n",
    "        times_gpu['copy to device'] = time.time() - t0\n",
    "        # Add on GPU\n",
    "        t0 = time.time()\n",
    "        result = add_gpu(x, y)\n",
    "        times_gpu['compute'] = time.time() - t0\n",
    "        # Copy data back\n",
    "        t0 = time.time()\n",
    "        x.copy_to_host(a)\n",
    "        y.copy_to_host(b)\n",
    "        result.copy_to_host(c)\n",
    "        times_gpu['copy to host'] = time.time() - t0\n",
    "        # cleanup\n",
    "        t0 = time.time()\n",
    "        del x, y, result\n",
    "        times_gpu['delete'] = time.time() - t0\n",
    "    del a, b, c\n",
    "    gpu_times[grid_size] = times_gpu.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(gpu_times).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_compute_times = [j['compute'] for j in gpu_times.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot([2**scale for scale in range(6, 14)], times['add_parallel'], label='add_parallel')\n",
    "ax.plot([2**scale for scale in range(6, 14)], times['add2'], label='add2')\n",
    "ax.plot([2**scale for scale in range(6, 13)], gpu_compute_times, label='add_gpu_compute')\n",
    "#ax.set_xticklabels(list(map(int, 2**ax.get_xticks())))\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks([2**scale for scale in range(6, 14)])\n",
    "ax.set_xticklabels([2**scale for scale in range(6, 14)])\n",
    "#ax.set_yscale('log')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU version is slower due to copying of memory from RAM to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at CUDA compilation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda install cudatoolkit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = setup(grid_size)\n",
    "b = setup(grid_size)\n",
    "t0 = time.time()\n",
    "_ = np.matmul(a, b)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform square matrix multiplication of C = A * B\"\"\"\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data\n",
    "grid_size = (8_000, 8_000)\n",
    "dtype = np.float64\n",
    "# Create Data\n",
    "a = setup(grid_size)\n",
    "b = setup(grid_size)\n",
    "c = np.zeros(grid_size)\n",
    "# Define external parameters\n",
    "segment_size = np.prod(grid_size)\n",
    "# Define Array\n",
    "times = {}\n",
    "#with cuda.gpus[0]:\n",
    "if grid_size[0]:\n",
    "    t0 = time.time()\n",
    "    x = cuda.device_array(grid_size, dtype=dtype) #you explicitly should type these\n",
    "    y = cuda.device_array(grid_size, dtype=dtype) #you explicitly should type these\n",
    "    result = cuda.device_array(grid_size, dtype=dtype) #you explicitly should type these\n",
    "    times['allocate memory'] = time.time() - t0\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        x.copy_to_device(a)\n",
    "        y.copy_to_device(b)\n",
    "        times['copy to device'] = time.time() - t0\n",
    "        t0 = time.time()\n",
    "        matmul[1, 1](x, y, result)\n",
    "        times['compute'] = time.time() - t0\n",
    "        t0 = time.time()\n",
    "        x.copy_to_host(a)\n",
    "        y.copy_to_host(b)\n",
    "        result.copy_to_host(c)\n",
    "        times['copy to host'] = time.time() - t0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        t0 = time.time()\n",
    "        del x, y, result\n",
    "        times['delete'] = time.time() - t0\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data\n",
    "grid_size = (8_000, 8_000)\n",
    "dtype = np.float64\n",
    "# Create Data\n",
    "a = setup(grid_size)\n",
    "b = setup(grid_size)\n",
    "c = np.zeros(grid_size)\n",
    "# Define external parameters\n",
    "segment_size = np.prod(grid_size)\n",
    "# Define Array\n",
    "times = {}\n",
    "#with cuda.gpus[0]:\n",
    "if grid_size[0]:\n",
    "    t0 = time.time()\n",
    "    x = cuda.device_array(grid_size, dtype=dtype) #you explicitly should type these\n",
    "    y = cuda.device_array(grid_size, dtype=dtype) #you explicitly should type these\n",
    "    result = cuda.device_array(grid_size, dtype=dtype) #you explicitly should type these\n",
    "    times['allocate memory'] = time.time() - t0\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        x.copy_to_device(a)\n",
    "        y.copy_to_device(b)\n",
    "        times['copy to device'] = time.time() - t0\n",
    "        t0 = time.time()\n",
    "        matmul[1, 1](x, y, result)\n",
    "        times['compute'] = time.time() - t0\n",
    "        t0 = time.time()\n",
    "        x.copy_to_host(a)\n",
    "        y.copy_to_host(b)\n",
    "        result.copy_to_host(c)\n",
    "        times['copy to host'] = time.time() - t0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        t0 = time.time()\n",
    "        del x, y, result\n",
    "        times['delete'] = time.time() - t0\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "- open  source  library  for  distributed  computing  in  Python\n",
    "- natively scale from a single computer to a cluster with little to no code alterations\n",
    "- provides  real-time  responsive  dashboards  for  progress  of  computations  using  Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask data structures:\n",
    "- definee collections analogous to: arrays,  bags,  dataframes\n",
    "- get operated  on  in  the  task-graph\n",
    "- distributed  by  either  the: Single  MachineScheduler  (SMS) or  the Distributed  Scheduler  (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Machine Scheduler (SMS)\n",
    "- threaded  scheduler  execution  via `multiprocessing.pool.ThreadPool`\n",
    "- lightweight framework with overhead $\\sim 50 \\mu s$ per task\n",
    "- requires no setup\n",
    "- incurs no data transfer delays\n",
    "- all computation occurs within  the  same  process  on  multiple  threads\n",
    "- due  to  the  Python Global  Interpreter Lock  (GIL),  parallelism  in  the  threaded  scheduler  can  only  occur  through  non-Python code  executions  (e.g.   through  NumPy,  Pandas,  Numba, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Scheduler (DS)\n",
    "- If  Python objects dominate computations,  a process-based scheduler should be used, here the distributed scheduler \n",
    "- Internally, the distributed scheduler determines the execution of the task graph with respect to locality in a more efficient way than traditional multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Parallel Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [d1[i:i*len(d1)//5].copy() for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with Pool(5) as p:\n",
    "    _ = p.map(expensive_vec, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processes = 2\n",
    "client = Client(processes=n_processes,\n",
    "                threads_per_worker=2,\n",
    "                n_workers=n_processes,\n",
    "                memory_limit='{}GB'.format(int(n_processes*2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgrei\\Anaconda3\\envs\\py38\\lib\\site-packages\\distributed\\worker.py:3373: UserWarning: Large object of size 1.80 MB detected in task graph: \n",
      "  ([248797.93748180202, 647650.4325437869, 291136.24 ... .23489052575],)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "futures = client.map(expensive_vec, chunks)\n",
    "bootstrap_results = np.array([fut.result() for fut in futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.36 s\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "big_futures = client.scatter(chunks)\n",
    "futures = client.submit(expensive_vec, big_futures)\n",
    "%time bootstrap_results = np.array([fut.result() for fut in big_futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(futures)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with parallel_backend('dask'):\n",
    "def proc(i):\n",
    "    # Your normal scikit-learn code here\n",
    "    from astroML.correlation import two_point_angular\n",
    "    if i > 0:\n",
    "        sample = np.sort(np.random.randint(0, len(x_dat), len(x_dat)))\n",
    "    else:\n",
    "        sample = range(len(x_dat))\n",
    "    x_sample = x_dat[sample]\n",
    "    y_sample = y_dat[sample]\n",
    "    bins = 10 ** np.linspace(np.log10(1/50000.), np.log10(0.5), 300)\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "    res = two_point_angular(x_sample, y_sample, bins=bins, method='landy-szalay')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(proc, range(n_bootstraps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results = np.array([fut.result() for fut in futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(futures)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining with Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
