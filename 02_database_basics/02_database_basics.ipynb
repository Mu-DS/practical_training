{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Management and Database Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "<img src=\"https://preview.redd.it/gph4rp6drvo41.jpg?width=640&crop=smart&auto=webp&s=a407a7be1da73ba010f0295a6351ab9d14471b2a\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "1. Pre-SQL  (Robin)\n",
    "2. SQL databases  (Emilio)\n",
    "3. Non-SQL databases  (Ali)\n",
    "4. Graph databases  (Simon, Day 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You kind of have data, but not really that much.\n",
    "- You want to organize it better,  but keep things lightweight to share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comma Separated Values (CSV) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Defined in 2005 by [RFC 4180](https://tools.ietf.org/html/rfc4180), the Request for Comments (RFC) publication of the Internet Society (ISOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Basics:\n",
    "- One table\n",
    "- Values separated by separator `,` (although `\\t, ;, etc.` also possible)\n",
    "- Entries separated by new lines `\\n` (although others also possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Benefits:\n",
    "- Human readability\n",
    "- Extremely efficient appending\n",
    "- Easy sharing between people and programs\n",
    "- Relatively fast reading and writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Drawbacks:\n",
    "- Only one table\n",
    "- Only tabular data\n",
    "- Disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Simplest CSV handling: Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "csv_test_file = \"C:/Users/rgrei/OneDrive/Work/Cryptix/data/Marketdata/boerse-frankfurt/bid-ask/XETR/sap-se.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### The Read function has *A LOT* of kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "pandas.read_csv(filepath_or_buffer, \n",
    "                sep=',', \n",
    "                delimiter=None, \n",
    "                header='infer', \n",
    "                names=None, \n",
    "                index_col=None, \n",
    "                usecols=None, \n",
    "                squeeze=False, \n",
    "                prefix=None, \n",
    "                mangle_dupe_cols=True, \n",
    "                dtype=None, engine=None, \n",
    "                converters=None, \n",
    "                true_values=None, \n",
    "                false_values=None, \n",
    "                skipinitialspace=False, \n",
    "                skiprows=None, \n",
    "                skipfooter=0, \n",
    "                nrows=None, \n",
    "                na_values=None, \n",
    "                keep_default_na=True, \n",
    "                na_filter=True, \n",
    "                verbose=False, \n",
    "                skip_blank_lines=True, \n",
    "                parse_dates=False, \n",
    "                infer_datetime_format=False, \n",
    "                keep_date_col=False, \n",
    "                date_parser=None, \n",
    "                dayfirst=False, \n",
    "                cache_dates=True, \n",
    "                iterator=False, \n",
    "                chunksize=None, \n",
    "                compression='infer', \n",
    "                thousands=None, \n",
    "                decimal='.', \n",
    "                lineterminator=None, \n",
    "                quotechar='\"', \n",
    "                quoting=0, \n",
    "                doublequote=True, \n",
    "                escapechar=None, \n",
    "                comment=None, \n",
    "                encoding=None, \n",
    "                dialect=None, \n",
    "                error_bad_lines=True, \n",
    "                warn_bad_lines=True, \n",
    "                delim_whitespace=False, \n",
    "                low_memory=True, \n",
    "                memory_map=False, \n",
    "                float_precision=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 601 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Define your `sep`arator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Allows to read all CSV-Type formats, e.g.:\n",
    "- Tab separated \"\\t\"\n",
    "- Space separated \" \"\n",
    "- Semicolon separated \";\"\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Ensuring vaslidity with `quotechar`, `escapechar`, and `dialect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`quotechar` defines your quote character to ensure proper parsing\n",
    "\n",
    "```python\n",
    "    quotechar='\"',\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`escapechar` defines a encoding escape character, e.g. `\\` such that `\\\\n` renders `\\n` instead of a new line\n",
    "\n",
    "```python\n",
    "    escapechar=None,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`dialect` defines parameters will \"override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting.\"\n",
    "\n",
    "```python\n",
    "    dialect=None,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Thousands and Fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    thousands=None, \n",
    "    decimal='.', \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Save CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(test.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reading ***large*** CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### File is compressed because it is too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    compression='infer', \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "compression:\n",
    "- \"on-the-fly decompression of on-disk data\"\n",
    "- default: `infer` compression from file-ending\n",
    "- other options: gzip, bz2, zip, xz, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Simple options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Skipping rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    skipinitialspace=False, \n",
    "    skiprows=None, \n",
    "    skipfooter=0, \n",
    "    nrows=None, \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Reading select columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    index_col=None, \n",
    "    usecols=None, \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Memory Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    low_memory=True, \n",
    "    memory_map=False,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`memory_map`\n",
    "- boolean attribute\n",
    "- \"map the file object directly onto memory\" for access\n",
    "- \"can improve performance because there is no longer any I/O overhead\"\n",
    "  - paging\n",
    "  - flushing\n",
    "- specifically:\n",
    "  - OS optimized subroutines are used\n",
    "  - strided memory access through numpy\n",
    "  - lazy loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 509 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(csv_test_file, memory_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Implicit mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`low_memory`\n",
    "- process the file in chunks internally\n",
    "- returns one full DataFrame\n",
    "- only chunk size used reducign memory bandwidth\n",
    "- allows for possibly mixed type inference within a column (!)\n",
    "    - manually set mixed types either to False\n",
    "    - or better specify the type with the dtype parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 459 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(csv_test_file, low_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Explicit Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    chunksize=None, \n",
    "    iteratorbool=False,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Iterating through files chunk by chunk by loading file lazily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`chunksize`:\n",
    "- number of lines ot be read as a chunk\n",
    "- returns `TextFileReader` object for iteration\n",
    "\n",
    "`iterator`:\n",
    "- Return `TextFileReader` object for iteration\n",
    "- etting chunks with `get_chunk()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.parsers.TextFileReader"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(csv_test_file, chunksize=4)\n",
    "a.get_chunk()\n",
    "a.get_chunk()\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For mor details see:\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "- https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Efficiently reading last lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sample file of > 20mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_file = \"C:/Users/rgrei/OneDrive/Work/Cryptix/data/Marketdata/boerse-frankfurt/bid-ask/XETR/sap-se.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Simple Read Last Line Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-24T11:58:31.11+02:00,138.82,138.78,419.0,510.0\n",
      "Wall time: 79.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(test_file, 'rb') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    last_line = lines[-1].decode('utf-8')\n",
    "    print(last_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_file' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(test_file, 'rb') as f:\n",
    "    f.seek(0, os.SEEK_END)\n",
    "    f.seek(f.tell() - 8*16, os.SEEK_SET)\n",
    "    lines = f.read().decode('utf-8').splitlines()[-1]\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([b'2020-08-24T11:58:31.11+02:00,138.82,138.78,419.0,510.0\\r\\n'], maxlen=1)\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(test_file, 'rb') as f:\n",
    "    last_line = deque(f, 1)\n",
    "    print(last_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2020-08-24T11:58:31.11+02:00,138.82,138.78,419.0,510.0\\r\\n'\n",
      "Wall time: 58.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(test_file, 'rb') as f:\n",
    "    for i in f:\n",
    "        pass\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Using seek offsets and buffer bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tail(f, window=1):\n",
    "    \"\"\"Returns the last `window` lines of file `f` as a list.\"\"\"\n",
    "    if window == 0:\n",
    "        return []\n",
    "\n",
    "    BUFSIZ = 8192\n",
    "    f.seek(0, 2)\n",
    "    remaining_bytes = f.tell()\n",
    "    size = window + 1\n",
    "    block = -1\n",
    "    data = []\n",
    "\n",
    "    while size > 0 and remaining_bytes > 0:\n",
    "        if remaining_bytes - BUFSIZ > 0:\n",
    "            # Seek back one whole BUFSIZ\n",
    "            f.seek(block * BUFSIZ, 2)\n",
    "            # read BUFFER\n",
    "            bunch = f.read(BUFSIZ)\n",
    "        else:\n",
    "            # file too small, start from beginning\n",
    "            f.seek(0, 0)\n",
    "            # only read what was not read\n",
    "            bunch = f.read(remaining_bytes)\n",
    "\n",
    "        bunch = bunch.decode('utf-8')\n",
    "        data.insert(0, bunch)\n",
    "        size -= bunch.count('\\n')\n",
    "        remaining_bytes -= BUFSIZ\n",
    "        block -= 1\n",
    "\n",
    "    return ''.join(data).splitlines()[-window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-08-24T11:58:31.11+02:00,138.82,138.78,419.0,510.0']\n",
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(test_file, 'rb') as f:\n",
    "    last_line = tail(f, 1)\n",
    "    print(last_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Appending CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-226-0b2e2bbb4f67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "with open(filename, \"a\"):\n",
    "    df.to_csv(filename, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Working with multiple CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since dictionaries are hashmaps, you can use them to map to files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Further, you can use them to map to memory mapped files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the `os` module, you can create nested dictionaries that memory map a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The result:\n",
    "\n",
    "> Simple access to various large data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Implementation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folderpath = \"C:/Users/rgrei/OneDrive/Work/Cryptix/data/Marketdata/boerse-frankfurt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-327-7f873dd1b5d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_dic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_folder_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolderpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-327-7f873dd1b5d4>\u001b[0m in \u001b[0;36mmap_folder_csv\u001b[1;34m(folderpath)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_folder_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-327-7f873dd1b5d4>\u001b[0m in \u001b[0;36mmap_folder_csv\u001b[1;34m(folderpath)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_folder_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-327-7f873dd1b5d4>\u001b[0m in \u001b[0;36mmap_folder_csv\u001b[1;34m(folderpath)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{folderpath}/{file}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'.csv'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mnew_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_folder_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \"\"\"\n\u001b[0;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def map_folder_csv(folderpath):\n",
    "    \"\"\"creates a tree hashmap to the memory mapped csv files (identical structure)\"\"\"\n",
    "    files = os.listdir(folderpath)\n",
    "    new_dic = {}\n",
    "    for file in files:\n",
    "        filepath = f\"{folderpath}/{file}\"\n",
    "        if '.csv' in file:\n",
    "            new_dic[file[:-4]] = pd.read_csv(filepath, memory_map=True)\n",
    "        elif os.path.isdir(filepath):\n",
    "            new_dic[file] = map_folder_csv(filepath)\n",
    "        else:\n",
    "            pass\n",
    "    return new_dic\n",
    "a = map_folder_csv(folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def process_filepath(filepath, fileending, reader):\n",
    "    \"\"\"memory map the files\"\"\"\n",
    "    if fileending in filepath:\n",
    "        return reader(filepath)\n",
    "    elif os.path.isdir(filepath):\n",
    "        return map_folder(filepath)\n",
    "    else:\n",
    "        return \n",
    "\n",
    "\n",
    "def map_folder(folderpath, fileending='.csv', reader=lambda x: pd.read_csv(x, memory_map=True)):\n",
    "    \"\"\"creates a tree hashmap to the memory mapped csv files (identical structure)\"\"\"\n",
    "    return {file.replace(fileending, ''): process_filepath(f\"{folderpath}/{file}\", fileending, reader)\n",
    "            for file in os.listdir(folderpath)}\n",
    "\n",
    "a = map_folder(folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "json_test_file = \"C:/Users/rgrei/OneDrive/Work/Cryptix/data/Marketdata/boerse-frankfurt/bond_definitions.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### JavaScript Object Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Defined by [RFC 7159](https://tools.ietf.org/html/rfc7159.html) (previously [RFC 4627](https://tools.ietf.org/html/rfc4627.html) and [ECMA-404](http://www.ecma-international.org/publications/standards/Ecma-404.htm)\n",
    "\n",
    "Standardized in 2013, current version in 2017 [RFC 8259](https://tools.ietf.org/html/rfc8259)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Benefits:\n",
    "- Human readbility\n",
    "- Easy sharing between people and programs\n",
    "- Relatively fast reading and writing\n",
    "- Hierarchies native"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Drawbacks:\n",
    "- Inefficient Storage and Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ignores indent so can be both easily human readable and more compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### JSON to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "JSON          | Python\n",
    "--------------|--------\n",
    "object        | dict\n",
    "array         | list\n",
    "string        | str\n",
    "number (int)  | int\n",
    "number (real) | float\n",
    "true          | True\n",
    "false         | False\n",
    "null          | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Python to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Python                                 | JSON\n",
    "---------------------------------------|------\n",
    "dict                                   | object\n",
    "list, tuple                            | array\n",
    "str                                    | string\n",
    "int, float, int- & float-derived Enums | number\n",
    "True                                   | true\n",
    "False                                  | false\n",
    "None                                   | null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### From file to Python object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Deserialize fp (a .read()-supporting text file or binary file containing a JSON document) to a Python object using this conversion table.\"\n",
    "\n",
    "```python\n",
    "json.load(fp, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "jf = json.load(open(json_test_file, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### From string to Python native dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Deserialize s (a str, bytes or bytearray instance containing a JSON document) to a Python object using this conversion table.\"\n",
    "\n",
    "```python\n",
    "json.loads(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'researcher': {'name': 'Ford Prefect', 'species': 'Betelgeusian', 'relatives': [{'name': 'Zaphod Beeblebrox', 'species': 'Betelgeusian'}]}}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = \"\"\"\n",
    "{\n",
    "    \"researcher\": {\n",
    "        \"name\": \"Ford Prefect\",\n",
    "        \"species\": \"Betelgeusian\",\n",
    "        \"relatives\": [\n",
    "            {\n",
    "                \"name\": \"Zaphod Beeblebrox\",\n",
    "                \"species\": \"Betelgeusian\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "data = json.loads(json_string)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Convert dictionary object to string for JSON storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Serialize obj to a JSON formatted str using this conversion table. The arguments have the same meaning as in dump().\"\n",
    "\n",
    "```python\n",
    "json.dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"researcher\": {\"name\": \"Ford Prefect\", \"species\": \"Betelgeusian\", \"relatives\": [{\"name\": \"Zaphod Beeblebrox\", \"species\": \"Betelgeusian\"}]}}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data))\n",
    "print(type(json.dumps(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Saving object as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Serialize obj as a JSON formatted stream to fp (a .write()-supporting file-like object) using this conversion table.\"\n",
    "\n",
    "```python\n",
    "json.dump(obj, fp, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('test_file.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### More on working with JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- https://realpython.com/python-json/\n",
    "- https://docs.python.org/3/library/json.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extensible Markup Language (XML) Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First Published 1998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Python XML module in the core library: https://docs.python.org/3/library/xml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a big warning:\n",
    "```\n",
    "Warning\n",
    "The XML modules are not secure against erroneous or maliciously constructed data. If you need to parse untrusted or unauthenticated data see the XML vulnerabilities and The defusedxml and defusedexpat Packages sections. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```xml\n",
    "<note>\n",
    "<to>Tove</to>\n",
    "<from>Jani</from>\n",
    "<heading>Reminder</heading>\n",
    "<body>Don't forget me this weekend!</body>\n",
    "</note>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### YAML Ain't Markup Language Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Introduced in 2001\n",
    "- Supposed to be a human readable format\n",
    "- Used for configuration files often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "```yaml\n",
    "---\n",
    "receipt:     Oz-Ware Purchase Invoice\n",
    "date:        2012-08-06\n",
    "customer:\n",
    "    first_name:   Dorothy\n",
    "    family_name:  Gale\n",
    "\n",
    "items:\n",
    "    - part_no:   A4786\n",
    "      descrip:   Water Bucket (Filled)\n",
    "      price:     1.47\n",
    "      quantity:  4\n",
    "\n",
    "    - part_no:   E1628\n",
    "      descrip:   High Heeled \"Ruby\" Slippers\n",
    "      size:      8\n",
    "      price:     133.7\n",
    "      quantity:  1\n",
    "\n",
    "bill-to:  &id001\n",
    "    street: |\n",
    "            123 Tornado Alley\n",
    "            Suite 16\n",
    "    city:   East Centerville\n",
    "    state:  KS\n",
    "\n",
    "ship-to:  *id001\n",
    "\n",
    "specialDelivery:  >\n",
    "    Follow the Yellow Brick\n",
    "    Road to the Emerald City.\n",
    "    Pay no attention to the\n",
    "    man behind the curtain.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Introduced in 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"[Apache Parquet](http://parquet.apache.org/) is a [columnar storage format](http://en.wikipedia.org/wiki/Column-oriented_DBMS) available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Python uses pyarrow to acces parquet files\n",
    "\n",
    "```bash\n",
    "pip install pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Converting to parquet compatible formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "time: string\n",
       "bestAskPrice: double\n",
       "bestBidPrice: double\n",
       "bestAskUnits: double\n",
       "bestBidUnits: double"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pa.Table.from_pandas(df)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Writing Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv:     18.8 MB, \n",
      "parquet:  3.6 MB\n"
     ]
    }
   ],
   "source": [
    "import humanize\n",
    "print(f\"csv:     {humanize.naturalsize(os.path.getsize(csv_test_file))}, \\nparquet:  {humanize.naturalsize(os.path.getsize('example.parquet'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "time: string\n",
       "bestAskPrice: double\n",
       "bestBidPrice: double\n",
       "bestAskUnits: double\n",
       "bestBidUnits: double"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = pq.read_table('example.parquet')\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>bestAskPrice</th>\n",
       "      <th>bestBidPrice</th>\n",
       "      <th>bestAskUnits</th>\n",
       "      <th>bestBidUnits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-10T13:58:30.42+02:00</td>\n",
       "      <td>135.600006</td>\n",
       "      <td>135.56</td>\n",
       "      <td>570.0</td>\n",
       "      <td>694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-10T13:58:30.88+02:00</td>\n",
       "      <td>135.600006</td>\n",
       "      <td>135.56</td>\n",
       "      <td>570.0</td>\n",
       "      <td>622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-10T13:58:34.81+02:00</td>\n",
       "      <td>135.600006</td>\n",
       "      <td>135.56</td>\n",
       "      <td>570.0</td>\n",
       "      <td>694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-10T13:58:36.06+02:00</td>\n",
       "      <td>135.600000</td>\n",
       "      <td>135.56</td>\n",
       "      <td>488.0</td>\n",
       "      <td>816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-10T13:58:36.06+02:00</td>\n",
       "      <td>135.600000</td>\n",
       "      <td>135.56</td>\n",
       "      <td>488.0</td>\n",
       "      <td>816.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           time  bestAskPrice  bestBidPrice  bestAskUnits  \\\n",
       "0  2020-08-10T13:58:30.42+02:00    135.600006        135.56         570.0   \n",
       "1  2020-08-10T13:58:30.88+02:00    135.600006        135.56         570.0   \n",
       "2  2020-08-10T13:58:34.81+02:00    135.600006        135.56         570.0   \n",
       "3  2020-08-10T13:58:36.06+02:00    135.600000        135.56         488.0   \n",
       "4  2020-08-10T13:58:36.06+02:00    135.600000        135.56         488.0   \n",
       "\n",
       "   bestBidUnits  \n",
       "0         694.0  \n",
       "1         622.0  \n",
       "2         694.0  \n",
       "3         816.0  \n",
       "4         816.0  "
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For more information on pyarrow: https://arrow.apache.org/docs/python/parquet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is also the popular: [FastParquet](https://fastparquet.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also read and write straight from pandas if you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## intro\n",
    " - sql is a declarative programming language to manipulate tables\n",
    " - declarative: no functions or loops, just _declare_ what you need and the runtime will figure out how to compute it\n",
    " - sql queries can be used to\n",
    "   - insert new rows into a table\n",
    "   - delete rows from a table\n",
    "   - update one or more attributes of one or more rows in a table\n",
    "   - retrieve and possibly transform rows combing from one or more tables\n",
    " - this section will mostly focus on reading data (last point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## main abstraction: tables\n",
    " - a table is a _set_ of tuples (rows)\n",
    "   - no two rows are the same\n",
    " - rows are distinguished by _primary keys_\n",
    "   - primary key: smallest set of attributes that uniquely identifies a row, examples:\n",
    "     - student ID (one attribute)\n",
    "     - first name, last name, birth date, place of birth (four attributes)\n",
    "   - the primary key is a property of each table\n",
    "     - all rows in a table use the same attributes as primary key\n",
    "     - but different tables can have different primary keys\n",
    "   - cannot have two rows with the same primary key\n",
    " - _foreign keys_ are used to refer to rows of other tables\n",
    "   - e.g. a table with grades will have foreign keys that point to the student and the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## domain\n",
    " - good database design has\n",
    "   - one table for each \"entity\" in the domain\n",
    "   - relationships between entities\n",
    " - types of binary relationships:\n",
    "   - 1 to 1 (can be stored in either entity, but NOT BOTH, or in a separate talbe)\n",
    "   - 1 to n (must be stored in the entity with cardinality \"1\", or in a separate table)\n",
    "   - m to n (requires a separate table)  \n",
    " - possible to have relationships between more than two entities\n",
    " - example:\n",
    "   - entities\n",
    "     - students (id, name, degree)\n",
    "     - courses (id, faculty, semester)\n",
    "     - professors (id, name, chair)\n",
    "   - relationships\n",
    "     - \"mentor\" between students (suppose 1 to 1), three possibilities\n",
    "       - have a column \"mentor\"\n",
    "       - have a column \"mentee\"\n",
    "       - having both is not ideal: more work to ensure consistency\n",
    "       - have a new table (mentor, mentee)\n",
    "     -  grades (entity student, entity course, attribute grade)\n",
    "       - m to n -> requires a table\n",
    "     - teaches (professors, courses)\n",
    "       - m to n -> requires a table\n",
    "       - only one professor per course -> store professor in course table or create separate table\n",
    " - sql shines when \"navigating\" across relationships, for example:\n",
    "   - for each student, find the professor that gave them the highest grade\n",
    "   - for each professor, find courses taught last semester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## anatomy of a select query\n",
    " - \"select\" queries are used to retrieve data from the database\n",
    "   \n",
    "    ```\n",
    "SELECT <columns and transformation>\n",
    "FROM <source table(s)>\n",
    "[WHERE <filter rows coming from source table(s)>]\n",
    "[GROUP BY <create groups of rows>\n",
    "[HAVING <filter groups>]]\n",
    "    ```\n",
    "    \n",
    " - must have SELECT+FROM\n",
    " - WHERE and GROUPBY optional\n",
    " - HAVING optional, must be used with GROUP BY\n",
    " - note on GROUP BY: eventually you must have only one row per group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## select query untangled\n",
    " - confusingly, order of execution is different than order of writing:\n",
    "   1. FROM: first, gather all input rows from all tables\n",
    "   2. WHERE: next, remove all rows not matching the predicate\n",
    "   3. GROUP BY: now, if needed, create groups of rows\n",
    "   4. HAVING: then, remove all groups that do not match the predicate\n",
    "   5. SELECT: finally, produce output rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FROM: source tables\n",
    " - you can specify one or more tables in the from clause\n",
    " - FROM will do a cross-product of all tuples of all tables\n",
    " - in almost all cases, you only want a small subset of the cross-product\n",
    "   - use WHERE to remove tuples that do not make sense - possible to give aliases to tables that can be used in the remainder of the query\n",
    " - possible to give aliases to tables and use that alias in the rest of the query\n",
    "   - useful to keep query short and when the same table is used several times in the same query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WHERE: tuple filter\n",
    " - specify a boolean condition that is evaluated for each row produced by the FROM\n",
    " - all rows where this evaluates to false are discarded\n",
    " - handling of null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JOIN: a special case of FROM+WHERE\n",
    " - in most cases, we are not interested in the cross-product\n",
    " - we actually want tuples that match primary/foreign keys\n",
    " - example `SELECT * FROM students, grades WHERE students.id = grade.student`\n",
    "   - associates to each student all its grades (one per row)\n",
    " - this operation is so common that it has a special name to distinguish it from the general case\n",
    " - `SELECT * FROM students JOIN grades ON student.id = grade.student`\n",
    " - options to handle non-matches:\n",
    "   - inner join: `FROM students [INNER] JOIN grades ON student.id = grade.student`\n",
    "     - `WHERE students.id = grade.student`\n",
    "     - only keep matches\n",
    "   - left join: `FROM students LEFT JOIN grades ON student.id = grade.student`\n",
    "     - `WHERE students.id = grade.student OR grade.student IS NULL`\n",
    "     - keep matches and un-matched records from _left_ table\n",
    "   - right join: `FROM students RIGHT JOIN grades ON student.id = grade.student`\n",
    "     - `WHERE students.id = grade.student OR stude=nt.id IS NULL`\n",
    "     - keep matches and un-matched records from _right_ table\n",
    "   - outer join: `FROM students OUTER JOIN grades ON student.id = grade.student`\n",
    "     - `WHERE students.id = grade.student OR grade.student IS NULL OR student.id IS NULL`\n",
    "     - keep matches, cross-product between un-matched records\n",
    " - other possibilities:\n",
    "    - natural join: `FROM students JOIN grades`\n",
    "      - `ON` is missing -> match all columns with the same name\n",
    "    - self join: `FROM stdudents s JOIN students t`\n",
    "      - better to use aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GROUP BY: create groups of rows\n",
    " - must specify one or more columns, possibly with transformation\n",
    " - all rows that have the same values for all (transformed) column(s) end up in the same group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HAVING: filter groups\n",
    " - another boolean condition applied to each group\n",
    " - example: filter by group size, min/max/mean of something.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SELECT: produce output columns\n",
    " - all the surviving groups/rows are transformed\n",
    " - select only a subset of attributes, or transform values\n",
    " - careful: each group must be collapsed into a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## subqueries and CTE\n",
    " - to make things messy\n",
    " - too many CTE's can make query slower, sometimes better to create temporary table\n",
    " - jww's usecase:\n",
    "    ```sql\n",
    "select t.id, (\n",
    "  select u.status\n",
    "  from tbl u\n",
    "  where t.id == u.id and t.timestamp >= u.timestamp\n",
    "  order by desc u.timestamp\n",
    "  limit 1\n",
    ") as status\n",
    "from tbl t\n",
    "```\n",
    "\n",
    "   - tbl has columns id, timestamp, status, where status can be null.\n",
    "   - goal is to fill null status with most recent non-null status (of the same id)\n",
    "   - need index on (id, timestamp) to be quick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## examples of complex queries\n",
    " - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## programmatically interfacing to a RDBMS\n",
    " - connections\n",
    " - cursors\n",
    " - sql injection and proper escaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## transactions and ACID\n",
    " - heh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## advanced: indexing\n",
    " - depending on your query and how you express it, it may be quite slow\n",
    " - the DBMS tries to optimize every query, but sometimes it fails\n",
    " - when most of the time is spent on joins and lookups, creating _indices_ can greatly speed up the query\n",
    " - an index is just a mapping from values to rows that contain that value in one or more columns\n",
    " - this makes it much faster to find rows that contain a given value\n",
    "   - instead of checking row by row, simply look in the index\n",
    "   - think about books!\n",
    " - an index is always relative to a table and one or more columns\n",
    "   - `CREATE INDEX <index name> ON <table name>(<list of columns>)`\n",
    " - a table can have many indices, but one is always created automatically for primary keys\n",
    "   - all otherunique keys must also have an index\n",
    "   - joins are much faster when there is an index on one of the columns\n",
    " - if a query is slow and/or executed very frequently, consider adding an index on columns used in the WHERE/JOIN\n",
    " - types of index:\n",
    "   - tree-based: O(NlogN) access, can be used to quickly answer queries like `WHERE L < column < U`\n",
    "   - hash-based: O(1) access, cannot answer range queries\n",
    "   - clustered index: table is physically sorted by the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## advanced: query plans\n",
    " - understanding why a query is slow is not trivial\n",
    " - the query plan is produced by the optimizer and shows exactly what and how is done to execute the query\n",
    " - it contains an estimated cost and can be augmented with the actual cost measured when executing the query\n",
    " - estimated cost:\n",
    "   - computed from statistics about rows/values that the DBMS maintains internally\n",
    "   - these statistics can become inaccurate after lots of operations\n",
    "   - useful to periodically recompute these statistics\n",
    "   - also useful to periodically clear the space allocated to deleted rows and defragment table data\n",
    " - (show example of plans before/after adding an index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "![](../img/qplan.png)\n",
    "\n",
    "Image from [dba.stackexchange.com](https://dba.stackexchange.com/q/9234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-SQL\n",
    "\n",
    "What does NoSQL actually mean?\n",
    "\n",
    "A bit of history …\n",
    "- 1998: First used for a relational database that omitted usage of SQL\n",
    "- 2009: First used during a conference to advocate non-relational databases\n",
    "\n",
    "NoSQL is an accidental term with no precise definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NoSQL: Overview\n",
    "\n",
    "Main objective: implement distributed state\n",
    "\n",
    "- Different objects stored on different servers\n",
    "- Same object replicated on different servers\n",
    "\n",
    "Main idea: give up some of the ACID constraints to improve performance\n",
    "\n",
    "Simple interface:\n",
    "\n",
    "- Write (=Put): needs to write all replicas\n",
    "- Read (=Get): may get only one\n",
    "\n",
    "Eventual consistency <- Strong consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NoSQL Six key features\n",
    "\n",
    "1. Scale horizontally “simple operations”\n",
    "2. Replicate/distribute data over many servers\n",
    "3. Simple call level interface (contrast w/ SQL)\n",
    "4. Weaker concurrency model than ACID\n",
    "5. Efficient use of distributed indexes and RAM\n",
    "6. Flexible schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of NoSQL Databases\n",
    "\n",
    "Core types\n",
    "- Key-value stores\n",
    "- Document stores\n",
    "- Wide column (column family, column oriented, …) stores\n",
    "- Graph databases\n",
    "\n",
    "Non-core types\n",
    "- Object databases\n",
    "- Native XML databases\n",
    "- RDF stores\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key-Value Stores\n",
    "\n",
    "Data model\n",
    "- he most simple NoSQL database type\n",
    "    - Works as a simple hash table (mapping)\n",
    "- Key-value pairs\n",
    "    - Key (id, identifier, primary key)\n",
    "    - Value: binary object, black box for the database system\n",
    "\n",
    "Query patterns\n",
    "- Create, update or remove value for a given key\n",
    "- Get value for a given key\n",
    "\n",
    "Characteristics\n",
    "- Simple model ⇒ great performance, easily scaled, …\n",
    "- Simple model ⇒ not for complex queries nor complex data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key-Value Stores\n",
    "\n",
    "Suitable use cases\n",
    "- Session data, user profiles, user preferences, shopping carts, …\n",
    "I.e. when values are only accessed via keys\n",
    "\n",
    "When not to use\n",
    "- Relationships among entities\n",
    "- Queries requiring access to the content of the value part\n",
    "- Set operations involving multiple key-value pairs\n",
    "\n",
    "Representatives\n",
    "- Redis, MemcachedDB, Riak KV, Hazelcast, Ehcache, Amazon, SimpleDB, Berkeley DB, Oracle NoSQL, Infinispan, LevelDB, Ignite, Project Voldemort\n",
    "- Multi-model: OrientDB, ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key-Value Stores\n",
    "\n",
    "<img src=\"photos/keyvalue_example.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key-Value Stores Use cases\n",
    "\n",
    "Key-value data stores are ideal for storing user profiles, blog comments, product recommendations, and session information.\n",
    "\n",
    "- Twitter uses Redis to deliver your Twitter timeline\n",
    "- Pinterest uses Redis to store lists of users, followers, unfollowers, boards, and more\n",
    "- Coinbase uses Redis to enforce rate limits and guarantee correctness of Bitcoin transactions\n",
    "- Quora uses Memcached to cache results from slower, persistent databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Stores\n",
    "\n",
    "Data model\n",
    "- Documents\n",
    "    - Self-describing\n",
    "    - Hierarchical tree structures (JSON, XML, …)\n",
    "        – Scalar values, maps, lists, sets, nested documents, …\n",
    "    - Identified by a unique identifier (key, …)\n",
    "- Documents are organized into collections\n",
    "\n",
    "Query patterns\n",
    "- Create, update or remove a document\n",
    "- Retrieve documents according to complex query conditions\n",
    "\n",
    "Observation\n",
    "- Extended key-value stores where the value part is examinable!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Stores\n",
    "\n",
    "Suitable use cases\n",
    "- Event logging, content management systems, blogs, web, analytics, e-commerce applications, …\n",
    "    - I.e. *for structured documents with similar schema*\n",
    "\n",
    "When not to use\n",
    "- *Set operations* involving multiple documents\n",
    "- Design of document structure is constantly changing\n",
    "    - I.e. when the required level of granularity would outbalance the advantages of aggregates\n",
    "\n",
    "Representatives\n",
    "- MongoDB, Couchbase, Amazon DynamoDB, CouchDB, RethinkDB, RavenDB, Terrastore\n",
    "- Multi-model: MarkLogic, OrientDB, OpenLink Virtuoso, ArangoDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Stores\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"year\" : 2013,\n",
    "        \"title\" : \"Turn It Down, Or Else!\",\n",
    "        \"info\" : {\n",
    "            \"directors\" : [ \"Alice Smith\", \"Bob Jones\"],\n",
    "            \"release_date\" : \"2013-01-18T00:00:00Z\",\n",
    "            \"rating\" : 6.2,\n",
    "            \"genres\" : [\"Comedy\", \"Drama\"],\n",
    "            \"image_url\" : \"http://ia.media-imdb.com/images/N/O9ERWAU7FS797AJ7LU8HN09AMUP908RLlo5JF90EWR7LJKQ7@@._V1_SX400_.jpg\",\n",
    "            \"plot\" : \"A rock band plays their music at high volumes, annoying the neighbors.\",\n",
    "            \"actors\" : [\"David Matthewman\", \"Jonathan G. Neff\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"year\": 2015,\n",
    "        \"title\": \"The Big New Movie\",\n",
    "        \"info\": {\n",
    "            \"plot\": \"Nothing happens at all.\",\n",
    "            \"rating\": 0\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Stores Use Cases\n",
    "\n",
    "- SEGA uses MongoDB for handling 11 million in-game accounts\n",
    "- Cisco moved its VSRM (video session and research manager) platform to Couchbase to achieve greater scalability\n",
    "- Aer Lingus uses MongoDB with Studio 3T to handle ticketing and internal apps\n",
    "- Built on MongoDB, The Weather Channel’s iOS and Android apps deliver weather alerts to 40 million users in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wide Column Stores\n",
    "\n",
    "Data model\n",
    "- Column family (table)\n",
    "    - Table is a collectioon of similar rows (not necessarily identical)\n",
    "\n",
    "- Row\n",
    "    - Row is a collectioon of columns\n",
    "        - Should encompass a group of data that is accessed together\n",
    "    - Associated with a unique row key\n",
    "\n",
    "- Column\n",
    "    - Column consists of a column name and column value (and possibly other metadata records)\n",
    "    - Scalar values, but also flat sets, lists or maps may be allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wide Column Stores\n",
    "\n",
    "Query patterns\n",
    "- Create, update or remove a row within a given column family\n",
    "- Select rows according to a row key or simple conditions\n",
    "\n",
    "Warning\n",
    "- Wide column stores are not just a special kind of RDBMSs with a variable set of columns!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wide Column Stores\n",
    "\n",
    "Suitable use cases\n",
    "- Event logging, content management systems, blogs, …\n",
    "    - I.e. for structured flat data with similar schema\n",
    "\n",
    "When not to use\n",
    "- ACID transactions are required\n",
    "- Complex queries: aggregation (SUM, AVG, …), joining, …\n",
    "- Early prototypes: i.e. when database design may change\n",
    "\n",
    "Representatives\n",
    "- Apache Cassandra, Apache HBase, Apache Accumulo, Hypertable, Google Bigtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Wide Column Stores\n",
    "\n",
    "<img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2012.24.12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wide Column Stores Use Cases\n",
    "\n",
    "Column stores offer very high performance and a highly scalable architecture. Because they’re fast to load and query, they’ve been popular among companies and organizations dealing with big data, IoT, and user recommendation and personalization engines.\n",
    "\n",
    "- Spotify uses Cassandra to store user profile attributes and metadata about artists, songs, etc. for better personalization\n",
    "- Facebook initially built its revamped Messages on top of HBase, but is now also used for other Facebook services like the Nearby Friends feature and search indexing\n",
    "- Outbrain uses Cassandra to serve over 190 billion personalized content recommendations each month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Databases\n",
    "\n",
    "Data model\n",
    "- Property graphs\n",
    "    - Directed / undirected graphs, i.e. collections of …\n",
    "        - nodes (vertices) for real-world entities, and\n",
    "        - relationships (edges) between these nodes\n",
    "    - Both the nodes and relationships can be associated with additional properties\n",
    "\n",
    "Types of databases\n",
    "- Non-transactional = small number of very large graphs\n",
    "- Transactional = large number of small graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Databases\n",
    "\n",
    "Query patterns\n",
    "- Create, update or remove a node / relationship in a graph\n",
    "- Graph algorithms (shortest paths, spanning trees, …)\n",
    "- General graph traversals\n",
    "- Sub-graph queries or super-graph queries\n",
    "- Similarity based queries (approximate matching)\n",
    "\n",
    "Representatives\n",
    "- Neo4j, Titan, Apache Giraph, InfiniteGraph, FlockDB\n",
    "- Multi-model: OrientDB, OpenLink Virtuoso, ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Databases\n",
    "\n",
    "Suitable use cases\n",
    "- Social networks, routing, dispatch, and location-based, services, recommendation engines, chemical compounds, biological pathways, linguistic trees, …\n",
    "    - I.e. simply for __graph structures__\n",
    "\n",
    "When not to use\n",
    "- Extensive batch operations are required\n",
    "    - Multiple nodes / relationships are to be affected\n",
    "- Only too large graphs to be stored\n",
    "    - Graph distribution is difficult or impossible at all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Databases\n",
    "\n",
    "The image below shows how a relational database like MySQL works, which use memory-intensive and more complicated join operations to search entire tables to find a match:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/from_relational_model.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Databases\n",
    "\n",
    "Compare that to a graph database, which already predetermines relationships based on connected nodes, making queries much faster and more economical.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/relational_to_graph.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Databases Use Cases\n",
    "\n",
    "Graph databases are great for establishing data relationships especially when dealing with large data sets. They offer blazing fast query performance that relational databases cannot compete with, especially as data grows much deeper.\n",
    "\n",
    "- Walmart uses Neo4j to provide customers personalized, relevant product recommendations and promotions\n",
    "- Medium uses Neo4j to build their social graph to enhance content personalization\n",
    "- Cisco uses Neo4j to mine customer support cases to anticipate bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reasons to use a NoSQL database\n",
    "\n",
    "- The pace of development with NoSQL databases can be much faster than with a SQL database.\n",
    "- The structure of many different forms of data is more easily handled and evolved with a NoSQL database.\n",
    "- The amount of data in many applications cannot be served affordably by a SQL database.\n",
    "- The scale of traffic and need for zero downtime cannot be handled by SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Data\n",
    "\n",
    "<img src=\"https://media.makeameme.org/created/big-data-big-5ad56d.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Big Data\n",
    "\n",
    "- high Volume\n",
    "- high Velocity\n",
    "- high Variety\n",
    "- Veracity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Sources\n",
    "\n",
    "- Social media and networks\n",
    "- Scientific instruments\n",
    "- Mobile devices\n",
    "- Sensor technology and networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture Design Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "Our client is a well-known manufacturer who builds different types of robotic manipulators. These robots are used by different operators/companies all around the world. \n",
    "\n",
    "During the operation, each robot creates a log file which includes timestamps as well as information from different sensors.\n",
    "\n",
    "For example, this is a snapshot of the log file\n",
    "\n",
    "```\n",
    ".\n",
    ".\n",
    ".\n",
    "timestamp 10:20:00 X 100 Y 100 Z 100\n",
    "timestamp 10:20:01 T 5 R 6\n",
    "timestamp 10:20:02 X 101 Y 99 Z 99\n",
    "timestamp 10:20:03 X 102 Y 100 Z 99\n",
    "timestamp 10:20:04 T 7 R 6\n",
    "timestamp 10:20:05 X 100 Y 100 Z 99\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "```\n",
    "\n",
    "The logfile can contain differet sensors and sometimes, there are differences between robot sensors and how they are captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main Idea\n",
    "\n",
    "Our client wants to create a platform (website) for health-condition-monitoring of the robots. In particular, our client wants to create a platform where different operators of the robotic arms can upload the log files and get insights about the status of their robot and different parts of it. \n",
    "\n",
    "<img src=\"photos/cybernetics-1869205_1280.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Task\n",
    "\n",
    "Design an architecture for the front-end and the back-end. (on the abstract level)\n",
    "Consider that each logfile is around 100Mb and is in text format\n",
    "\n",
    "- What types of databases are needed?\n",
    "- In case of sql databases, what columns do you suggest?\n",
    "- Do we need any nosql db? what type?\n",
    "\n",
    "Note: in this case, there are no unique solution. Try to be creative :)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
