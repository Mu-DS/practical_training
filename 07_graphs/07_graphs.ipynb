{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Machine Learning for Graphs\n",
    "This notebook contains:\n",
    "1. A very brief introduction for the library [`NetworkX`](https://networkx.github.io/).\n",
    "1. An introduction to Graph Neural Networks (GNNs) with [`PyTorch`](https://pytorch.org/).\n",
    "1. Some basic queries for the proprietary graph database [`Neo4j`](https://neo4j.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NetworkX\n",
    "How to handle graph data in python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'NetworkX version used: {nx.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset ðŸ“ž â˜Žï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./log_of_calls.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to NetworkX graph\n",
    "It is sometimes some work to get all the edge and feature attributes into the desired format ðŸ¤¬!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_df = df[[c for c in df.columns if c.startswith('from_')]]\n",
    "from_df.columns = [c[5:] for c in from_df.columns]\n",
    "to_df = df[[c for c in df.columns if c.startswith('to_')]]\n",
    "to_df.columns = [c[3:] for c in to_df.columns]\n",
    "df_nodes = pd.concat((from_df, to_df), ignore_index=True)\n",
    "df_nodes = df_nodes.drop(columns='dt')\n",
    "df_nodes = df_nodes.drop_duplicates(subset='number')\n",
    "df_nodes = df_nodes.reset_index(drop=True)\n",
    "df_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.MultiDiGraph()\n",
    "\n",
    "G.add_nodes_from(zip(\n",
    "    df_nodes.number,\n",
    "    df_nodes.drop(columns='number').to_dict('records')\n",
    "))\n",
    "G.nodes(data=True)['403-726-6587']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of edges preserves the insertion order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.nodes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edges_from(zip(\n",
    "    df.from_number,\n",
    "    df.to_number,\n",
    "    df[['from_dt', 'to_dt']].to_dict('records')\n",
    "))\n",
    "list(G.edges(nbunch='403-726-6587', data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the Graph look like? ðŸ“Š ðŸ“ˆ ðŸ“‰\n",
    "As mentioned this question strongly depends on the spatial order of nodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "nx.draw_circular(G, width=0.1, node_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Summary Statistics ðŸ“‚\n",
    "Plenty of algorithms are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame  \n",
    "IFrame('https://networkx.github.io/documentation/networkx-2.4/reference', width=1000, height=650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks (GNNs) with PyTorch (aka Message Passing ðŸ“¥ ðŸ“¤)\n",
    "Graph Neural Networks (GNNs) unraveled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the labels ðŸ¤·â€â™‚ï¸ ðŸ¤·â€â™€ï¸\n",
    "_(This task is probably political correct, but the result is quite surprising!)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_nodes['gender'].values)\n",
    "y = torch.from_numpy(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the features\n",
    "I.e. the one hot encoding of names:\n",
    "\n",
    "**Observation:** \n",
    "ðŸ”Ž_The name is basically a unique identifier, so how should we be able to learn something (with partially labelled data)?_ ðŸ”Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = OneHotEncoder().fit_transform(df_nodes['name'].values[:, None]).toarray()\n",
    "X = torch.from_numpy(X).float()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the NetworkX graph into a sparse adjacecny matrix\n",
    "Otherwise the space requirements are $O(n^2)$ with the number of nodes $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_networkx_to_sparse_tensor(G: nx.Graph) -> torch.Tensor:\n",
    "    if hasattr(G, 'to_undirected'):\n",
    "        G = G.to_undirected()\n",
    "    adjacency_matrix = nx.convert_matrix.to_scipy_sparse_matrix(G)\n",
    "    adjacency_matrix += sp.diags(np.ones(len(G.nodes())))\n",
    "    adjacency_matrix = adjacency_matrix.tocoo()\n",
    "    row_index = torch.from_numpy(adjacency_matrix.row).to(torch.long)\n",
    "    col_index = torch.from_numpy(adjacency_matrix.col).to(torch.long)\n",
    "    A = torch.sparse.FloatTensor(\n",
    "        torch.stack([row_index, col_index], dim=0),\n",
    "        torch.ones_like(row_index, dtype=torch.float)\n",
    "    ).coalesce()\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = from_networkx_to_sparse_tensor(G)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a Graph Convolutional Network (GCN)\n",
    "For the graph convolutional layer we are going to use the following update scheme:\n",
    "\n",
    "$$ð»^{(ð‘™+1)}=\\sigma\\left(ð·^{âˆ’\\frac{1}{2}} ð´ ð·^{âˆ’\\frac{1}{2}} ð»^{(ð‘™)} ð‘Š{(ð‘™)}\\right)=\\sigma\\left(\\hat{A} ð»^{(ð‘™)} ð‘Š{(ð‘™)}\\right)$$\n",
    "\n",
    "We use the ReLU for the activation function, but in the last layer where we directly output the raw logits (i.e. no activation at all). With $ð»^{(0)}$ we denote the node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolution Layer: as proposed in [Kipf et al. 2017](https://arxiv.org/abs/1609.02907).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels: int\n",
    "        Dimensionality of input channels/features.\n",
    "    out_channels: int\n",
    "        Dimensionality of output channels/features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "\n",
    "    def forward(self, arguments: Tuple[torch.tensor, torch.sparse.FloatTensor]) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arguments: Tuple[torch.tensor, torch.sparse.FloatTensor]\n",
    "            Tuple of feature matrix `X` and normalized adjacency matrix `A_hat`\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        X: torch.tensor\n",
    "            The result of the message passing step\n",
    "        \"\"\"\n",
    "        X, A_hat = arguments\n",
    "        X = A_hat @ self.linear(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we stack multiple layers ($l$) (with ReLU activation functions and dropout in between). Before we pass the adjacency matrix to the GCN, we calculate the normalized adjacency matrix: \n",
    "$$\\hat{A} = ð·^{âˆ’\\frac{1}{2}} ð´ ð·^{âˆ’\\frac{1}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolution Network: as proposed in [Kipf et al. 2017](https://arxiv.org/abs/1609.02907).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features: int\n",
    "        Dimensionality of input features.\n",
    "    n_classes: int\n",
    "        Number of classes for the semi-supervised node classification.\n",
    "    hidden_dimensions: List[int]\n",
    "        Internal number of features. `len(hidden_dimensions)` defines the number of hidden representations.\n",
    "    activation: nn.Module\n",
    "        The activation for each layer but the last.\n",
    "    dropout: float\n",
    "        The dropout probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_features: int,\n",
    "                 n_classes: int,\n",
    "                 hidden_dimensions: List[int] = [80],\n",
    "                 activation: nn.Module = nn.ReLU(),\n",
    "                 dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend([\n",
    "            nn.Sequential(OrderedDict([\n",
    "                (f'gcn_{idx}', GraphConvolution(in_channels=in_channels,\n",
    "                                                out_channels=out_channels)),\n",
    "                (f'activation_{idx}', activation),\n",
    "                (f'dropout_{idx}', nn.Dropout(p=dropout))\n",
    "            ]))\n",
    "            for idx, (in_channels, out_channels)\n",
    "            in enumerate(zip([n_features] + hidden_dimensions[:-1], hidden_dimensions))\n",
    "        ])\n",
    "        self.layers.append(\n",
    "            nn.Sequential(OrderedDict([\n",
    "                (f'gcn_{len(hidden_dimensions)}', GraphConvolution(in_channels=hidden_dimensions[-1],\n",
    "                                                                  out_channels=n_classes))\n",
    "            ]))\n",
    "        )\n",
    "  \n",
    "    def normalize(self, A: torch.sparse.FloatTensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        For calculating $\\hat{A} = ð·^{âˆ’\\frac{1}{2}} ð´ ð·^{âˆ’\\frac{1}{2}}$.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A: torch.sparse.FloatTensor\n",
    "            Sparse adjacency matrix with added self-loops.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A_hat: torch.sparse.FloatTensor\n",
    "            Normalized message passing matrix\n",
    "        \"\"\"\n",
    "        row, col = A._indices()\n",
    "        edge_weight = A._values()\n",
    "        deg = (A @ torch.ones(A.shape[0], 1, device=A.device)).squeeze()\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        normalized_edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "        A_hat = torch.sparse.FloatTensor(A._indices(), normalized_edge_weight, A.shape)\n",
    "        return A_hat\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.sparse.FloatTensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.tensor\n",
    "            Feature matrix `X`\n",
    "        A: torch.tensor\n",
    "            adjacency matrix `A` (with self-loops)\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        X: torch.tensor\n",
    "            The result of the last message passing step (i.e. the logits)\n",
    "        \"\"\"\n",
    "        A_hat = self.normalize(A)\n",
    "        for layer in self.layers:\n",
    "            X = layer((X, A_hat))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation/Test split ðŸŽ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(labels: np.ndarray,\n",
    "          train_size: float = 0.1,\n",
    "          val_size: float = 0.1,\n",
    "          test_size: float = 0.8,\n",
    "          random_state: int = 42) -> List[np.ndarray]:\n",
    "    \"\"\"Split the arrays or matrices into random train, validation and test subsets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels: np.ndarray [n_nodes]\n",
    "        The class labels\n",
    "    train_size: float\n",
    "        Proportion of the dataset included in the train split.\n",
    "    val_size: float\n",
    "        Proportion of the dataset included in the validation split.\n",
    "    test_size: float\n",
    "        Proportion of the dataset included in the test split.\n",
    "    random_state: int\n",
    "        Random_state is the seed used by the random number generator;\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    split_train: array-like\n",
    "        The indices of the training nodes\n",
    "    split_val: array-like\n",
    "        The indices of the validation nodes\n",
    "    split_test array-like\n",
    "        The indices of the test nodes\n",
    "\n",
    "    \"\"\"\n",
    "    idx = np.arange(labels.shape[0])\n",
    "    idx_train_and_val, idx_test = train_test_split(idx,\n",
    "                                                   random_state=random_state,\n",
    "                                                   train_size=(train_size + val_size),\n",
    "                                                   test_size=test_size,\n",
    "                                                   stratify=labels)\n",
    "\n",
    "    idx_train, idx_val = train_test_split(idx_train_and_val,\n",
    "                                          random_state=random_state,\n",
    "                                          train_size=(train_size / (train_size + val_size)),\n",
    "                                          test_size=(val_size / (train_size + val_size)),\n",
    "                                          stratify=labels[idx_train_and_val])\n",
    "    \n",
    "    return idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training code... ðŸŽ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "          X: torch.Tensor, \n",
    "          A: torch.sparse.FloatTensor, \n",
    "          labels: torch.Tensor, \n",
    "          idx_train: np.ndarray, \n",
    "          idx_val: np.ndarray,\n",
    "          lr: float = 1e-2,\n",
    "          weight_decay: float = 5e-4, \n",
    "          patience: int = 50, \n",
    "          max_epochs: int = 500, \n",
    "          display_step: int = 10):\n",
    "    \"\"\"\n",
    "    Train a model using either standard or adversarial training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Model which we want to train.\n",
    "    X: torch.Tensor [n, d]\n",
    "        Dense attribute matrix.\n",
    "    A: torch.sparse.FloatTensor [n, n]\n",
    "        Sparse adjacency matrix.\n",
    "    labels: torch.Tensor [n]\n",
    "        Ground-truth labels of all nodes,\n",
    "    idx_train: np.ndarray [?]\n",
    "        Indices of the training nodes.\n",
    "    idx_val: np.ndarray [?]\n",
    "        Indices of the validation nodes.\n",
    "    lr: float\n",
    "        Learning rate.\n",
    "    weight_decay : float\n",
    "        Weight decay.\n",
    "    patience: int\n",
    "        The number of epochs to wait for the validation loss to improve before stopping early.\n",
    "    max_epochs: int\n",
    "        Maximum number of epochs for training.\n",
    "    display_step : int\n",
    "        How often to print information.\n",
    "    seed: int\n",
    "        Seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    trace_train: list\n",
    "        A list of values of the train loss during training.\n",
    "    trace_val: list\n",
    "        A list of values of the validation loss during training.\n",
    "    \"\"\"\n",
    "    trace_train = []\n",
    "    trace_val = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    for it in tqdm(range(max_epochs), desc='Training...'):\n",
    "        logits = model(X, A)     \n",
    "        loss_train = F.cross_entropy(logits[idx_train], labels[idx_train])\n",
    "        loss_val = F.cross_entropy(logits[idx_val], labels[idx_val])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        trace_train.append(loss_train.detach().item())\n",
    "        trace_val.append(loss_val.detach().item())\n",
    "\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_epoch = it\n",
    "            best_state = {key: value.cpu() for key, value in model.state_dict().items()}\n",
    "        else:\n",
    "            if it >= best_epoch + patience:\n",
    "                break\n",
    "\n",
    "        if display_step > 0 and it % display_step == 0:\n",
    "            print(f'Epoch {it:4}: loss_train: {loss_train.item():.5f}, loss_val: {loss_val.item():.5f} ')\n",
    "\n",
    "    # restore the best validation state\n",
    "    model.load_state_dict(best_state)\n",
    "    return trace_train, trace_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš§ Putting it all together ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, C = X.shape[1], y.max() + 1\n",
    "\n",
    "gcn = GCN(n_features=D, n_classes=C, hidden_dimensions=[64])\n",
    "\n",
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = split(y.numpy(), train_size=0.1, val_size=0.1, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace_train, trace_val = train(gcn, X, A, y, idx_train, idx_val)\n",
    "\n",
    "plt.plot(trace_train, label='train')\n",
    "plt.plot(trace_val, label='validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn.eval()\n",
    "logits = gcn(X, A)\n",
    "accuracy = (torch.argmax(logits, dim=-1) == y)[idx_test].float().mean()\n",
    "print(f'We can predict the name with an accuracy of {100*accuracy:.2f} % ' \n",
    "      f'based on non-informative features due to the graph stucture!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ðŸ—º Graph Databases (Neo4j)\n",
    "Handle your graph data professionally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Switch to this folder: `cd 07_graphs`\n",
    "\n",
    "Start Neo4j server e.g. via docker (initial user: `neo4j`, pw: `neo4j`):\n",
    "```bash\n",
    "docker run \\\n",
    "    --publish=7474:7474 --publish=7687:7687 \\\n",
    "    --volume=$PWD/data:/data \\\n",
    "    --volume=$PWD/import:/import \\\n",
    "    --env 'NEO4JLABS_PLUGINS=[\"graph-data-science\"]' \\\n",
    "    neo4j:4.1.1\n",
    "```\n",
    "\n",
    "Then connect to the Neo4j bowser via `http://localhost:7474/` (default user and pw is typically `neo4j`).\n",
    "\n",
    "### Load Graph\n",
    "\n",
    "```sql\n",
    "MATCH (n) DETACH DELETE n;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///log_of_calls.csv' AS line\n",
    "MERGE (c1:City { name: line.from_city })\n",
    "MERGE (p1:Person { name: line.from_name, number: line.from_number, gender: line.from_gender })\n",
    "MERGE (p1)-[:FROM]->(c1)\n",
    "MERGE (c2:City { name: line.to_city })\n",
    "MERGE (p2:Person { name: line.to_name, number: line.to_number, gender: line.to_gender })\n",
    "MERGE (p2)-[:FROM]->(c2)\n",
    "CREATE (p1)-[c:Calls { \n",
    "\t\tfrom: datetime(line.from_dt),\n",
    "\t\tto: datetime(line.to_dt),\n",
    "        duration: duration.between(datetime(line.from_dt), datetime(line.to_dt)).minutes\n",
    "\t}]->(p2)\n",
    "```\n",
    "\n",
    "### Visualize Graph\n",
    "\n",
    "For example we want to have a look at all persons from `Pattaya`:\n",
    "```sql\n",
    "MATCH p=()-[r:FROM]->({ name: 'Pattaya' }) \n",
    "RETURN p\n",
    "```\n",
    "or equivalently:\n",
    "```sql\n",
    "MATCH p=()-[r:FROM]->(c)\n",
    "WHERE c.name='Pattaya'\n",
    "RETURN p\n",
    "```\n",
    "\n",
    "### Explain\n",
    "Similarily to SQL, we can execute an `EXPLAIN` query for analyis of the execution plan:\n",
    "```\n",
    "EXPLAIN MATCH p=()-[r:FROM]->(c)\n",
    "WHERE c.name='Pattaya'\n",
    "RETURN p\n",
    "```\n",
    "\n",
    "### Closeness centrality\n",
    "```\n",
    "CALL gds.alpha.closeness.stream({\n",
    "  nodeProjection: 'Person',\n",
    "  relationshipProjection: 'Calls'\n",
    "})\n",
    "YIELD nodeId, centrality\n",
    "RETURN gds.util.asNode(nodeId).name AS user, centrality\n",
    "ORDER BY centrality DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
